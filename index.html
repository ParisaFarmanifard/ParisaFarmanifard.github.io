<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>Parisa Farmanifard</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <!-- Type pairing: Inter + Libre Baskerville -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;700;800&family=Libre+Baskerville:wght@400;700&display=swap" rel="stylesheet" />
  <style>
    :root{
      /* Light palette — tweak these for quick rebranding */
      --page: #ffffff;             /* page background */
      --paper: #ffffff;            /* card background */
      --ink: #17223b;              /* primary text */
      --muted: #4b5b76;           /* secondary text */
      --line: #e7ecf3;            /* card borders */
      --ring: rgba(31,122,255,.22);

      /* Cheerful accent set (used subtly for “colorful” feel) */
      --accent: #1f7aff;          /* blue */
      --accent-2: #7c3aed;        /* purple */
      --accent-3: #10b981;        /* green */
      --accent-4: #f59e0b;        /* amber */
      --accent-5: #ef4444;        /* red */

      /* Shadows & radii */
      --shadow-sm: 0 1px 2px rgba(16,24,40,.06);
      --shadow-md: 0 4px 14px rgba(16,24,40,.10);
      --shadow-lg: 0 10px 24px rgba(16,24,40,.12);
      --radius-lg: 18px;
      --radius-md: 12px;

      /* Hero gradient */
      --hero-grad: radial-gradient(1000px 600px at 10% 0%, rgba(31,122,255,.18), transparent 55%),
                   radial-gradient(800px 500px at 90% 0%, rgba(124,58,237,.16), transparent 60%),
                   linear-gradient(180deg, #f7fbff 0%, #ffffff 60%, #ffffff 100%);
      --stripe: linear-gradient(90deg,
                  rgba(31,122,255,.18), rgba(31,122,255,0) 22%,
                  rgba(124,58,237,.18) 22%, rgba(124,58,237,0) 44%,
                  rgba(16,185,129,.18) 44%, rgba(16,185,129,0) 66%,
                  rgba(245,158,11,.18) 66%, rgba(245,158,11,0) 88%,
                  rgba(239,68,68,.18) 88%, rgba(239,68,68,0) 100%);
    }

    *{ box-sizing: border-box; }
    html, body{ height:100%; }
    body{
      margin:0;
      font-family: Inter, system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial;
      color: var(--ink);
      background: var(--hero-grad);
      line-height: 1.65;
      -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale;
    }

    a{ color: var(--accent); text-decoration: none; }
    a:hover{ color: #0b63e6; text-decoration: underline; }

    .wrap{
      max-width: 1024px;
      margin: 0 auto;
      padding: 28px 18px 80px;
    }

    /* Header */
    .header{
      border: 1px solid var(--line);
      border-radius: var(--radius-lg);
      background: var(--paper);
      box-shadow: var(--shadow-md);
      padding: 34px 22px 22px;
      text-align: center;
      position: relative; overflow: hidden; isolation: isolate;
    }
    .header::after{
      /* subtle, colorful diagonal stripes in the header background */
      content:"";
      position:absolute; inset:0;
      background: var(--stripe);
      opacity: .35;
      mask-image: linear-gradient(to bottom, rgba(0,0,0,.25), rgba(0,0,0,0.06) 65%, transparent 90%);
      pointer-events:none; z-index:0;
    }

    .avatar{
      width: 160px; height:160px; border-radius: 50%;
      object-fit: cover;
      display:block; margin: 2px auto 14px;
      box-shadow: 0 10px 24px rgba(16,24,40,.22), 0 0 0 6px rgba(31,122,255,.08);
      transition: transform .3s ease, box-shadow .3s ease;
      position: relative; z-index: 1;
    }
    .avatar:hover{ transform: translateY(-2px); box-shadow: 0 14px 30px rgba(16,24,40,.26), 0 0 0 8px rgba(31,122,255,.12); }

    h1{
      margin: 6px 0 6px;
      font-family: "Libre Baskerville", Georgia, serif;
      font-weight: 700;
      font-size: clamp(30px, 4.6vw, 44px);
      letter-spacing: .2px;
      color:#0d1b2a;
      position: relative; z-index: 1;
    }

    .social{
      display:flex; justify-content:center; align-items:center; gap:16px; margin: 14px 0 0;
      position: relative; z-index: 1;
    }
    .social a{
      width:48px; height:48px; border-radius: 12px;
      border:1px solid var(--line);
      background: #ffffff;
      display:flex; align-items:center; justify-content:center;
      box-shadow: var(--shadow-sm);
      transition: transform .25s ease, border-color .25s ease, box-shadow .25s ease;
    }
    .social a:hover{
      transform: translateY(-2px);
      border-color: rgba(31,122,255,.35);
      box-shadow: var(--shadow-md);
    }
    .social img{ width:26px; height:26px; }

    /* Intro */
    .intro{
      margin-top: 20px;
      border: 1px solid var(--line);
      border-radius: var(--radius-md);
      background: var(--paper);
      box-shadow: var(--shadow-sm);
      padding: 18px 20px;
    }
    p{ margin: 0 0 10px; text-align: justify; color: var(--ink); }

    /* Section */
    .section{
      margin-top: 26px;
      border:1px solid var(--line);
      border-radius: var(--radius-lg);
      background: var(--paper);
      box-shadow: var(--shadow-md);
      padding: 20px 20px;
    }
    .section h3{
      margin: 0 0 12px;
      font-weight: 800; color:#0f172a;
      font-size: clamp(18px, 2.2vw, 22px);
      display:flex; align-items:center; gap:12px;
    }
    .section h3::after{
      content:""; flex:1; height:2px;
      background: linear-gradient(90deg, var(--accent), var(--accent-2), var(--accent-3), var(--accent-4), var(--accent-5));
      border-radius: 999px;
      opacity: .6;
    }

    /* Publications list */
    .pub{ display:grid; gap: 12px; }
    .pub-item{ position: relative; padding-left: 18px; }
    .pub-item::before{
      content:""; position:absolute; left:0; top:.9em;
      width:10px; height:10px; border-radius:50%;
      background: var(--accent);
      box-shadow:
        0 0 0 4px rgba(31,122,255,.18),
        16px 0 0 -6px var(--accent-2),
        28px 0 0 -10px var(--accent-3);
      opacity:.95;
    }
    .pub-meta{ color: var(--muted); font-size: 14px; }

    /* Grid projects */
    .grid{ display:grid; gap: 16px; grid-template-columns: repeat(12, 1fr); }
    .project{
      grid-column: 1 / -1;
      border:1px solid var(--line);
      border-radius: var(--radius-md);
      background: var(--paper);
      box-shadow: var(--shadow-md);
      padding: 16px;
      transition: transform .25s ease, box-shadow .25s ease, border-color .25s ease;
    }
    .project:hover{
      transform: translateY(-2px);
      border-color: rgba(31,122,255,.35);
      box-shadow: var(--shadow-lg);
    }
    .project h4{ margin: 0 0 6px; color:#0f172a; }

    .proj-body{
      display:grid; gap: 14px;
      grid-template-columns: 1fr;
      align-items: start; /* ensure columns align from the top */
    }

    /* Framework image frame + lightbox hook (light theme) */
    .frame{
      position: relative; width: 100%;
      /* removed fixed aspect-ratio to let image decide height */
      border:1px solid var(--line);
      border-radius: 12px;
      overflow: hidden;
      background: #f6f9ff;
      box-shadow: inset 0 1px 0 rgba(15,23,42,.04);
      cursor: zoom-in;
      margin: 0;
    }
    .frame::after{
      /* subtle multicolor bar at the top of images */
      content:""; position:absolute; left:0; top:0; right:0; height:4px;
      background: linear-gradient(90deg, var(--accent), var(--accent-2), var(--accent-3), var(--accent-4), var(--accent-5));
      opacity:.75;
    }
    .frame img{
      /* FIX: no absolute positioning; no cropping */
      position: static; 
      display: block;
      width: 100%;
      height: auto;              /* preserve natural aspect */
      object-fit: contain;       /* show full diagram */
      object-position: center;
      transition: transform .35s ease, filter .35s ease;
    }
    .frame:hover img{ transform: scale(1.02); filter: saturate(1.06) contrast(1.02); }

    /* Footer */
    footer{
      margin-top: 26px; text-align:center; padding: 16px;
      border:1px solid var(--line); border-radius: 12px;
      background: var(--paper);
      box-shadow: var(--shadow-sm);
      font-size: 15px;
    }

    /* Lightbox using dialog (light) */
    dialog.lightbox{
      padding:0; border: none; background: transparent; max-width: 94vw;
    }
    dialog::backdrop{
      background: rgba(15,23,42,.45);
      backdrop-filter: blur(2px);
    }
    .lightbox-img{
      max-width: 94vw; max-height: 90vh; display:block;
      border-radius: 12px; border:1px solid var(--line);
      box-shadow: 0 30px 70px rgba(16,24,40,.20);
      object-fit: contain;      /* keep full image in lightbox too */
    }

    /* Responsive */
    @media (min-width: 840px){
      .proj-body{ grid-template-columns: 5fr 7fr; align-items: start; }
    }
    @media (max-width: 640px){
      .avatar{ width:140px; height:140px; }
      .section, .intro{ padding:16px; }
    }
  </style>
</head>
<body>
  <div class="wrap">

    <!-- Header -->
    <header class="header">
      <img class="avatar" src="images/parisa_july2025.jpg" alt="Parisa Farmanifard" loading="lazy">
      <h1>Parisa Farmanifard</h1>
      <nav class="social" aria-label="Profiles">
        <a href="https://scholar.google.com/citations?user=q6iHSi8AAAAJ&hl=en&authuser=3" target="_blank" rel="noopener">
          <img src="images/googlescholar.png" alt="Google Scholar" loading="lazy">
        </a>
        <a href="https://github.com/ParisaFarmanifard" target="_blank" rel="noopener">
          <img src="images/github_icon.svg" alt="GitHub" loading="lazy">
        </a>
        <a href="https://www.linkedin.com/in/parisaf/" target="_blank" rel="noopener">
          <img src="images/linkedin_logo_icon.svg" alt="LinkedIn" loading="lazy">
        </a>
      </nav>
    </header>

    <!-- Intro (text unchanged) -->
    <section class="intro">
      <p>Welcome to my webpage! My name is Parisa Farmanifard, a Ph.D. student in the Department of Computer Science and Engineering at Michigan State University, where I conduct research in the <a href="https://iprobe.cse.msu.edu/index.php">iPRoBe Lab</a> under the guidance of <a href="https://scholar.google.com/citations?user=7IiUQDkAAAAJ&hl=en">Dr. Arun Ross</a>. I also earned my M.S. in Computer Science from MSU. My research focuses on biometrics, particularly iris recognition and presentation attack detection, with a growing emphasis on integrating foundation models (FMs) and large language models (LLMs) into biometric systems.</p>
    </section>

    <!-- Publications -->
    <section class="section" id="publications">
      <h3>Publications</h3>
      <div class="pub">
        <div class="pub-item">
          <p>[2025] M. Mitcheff, A. Hossain, S. Webster, S.K. Karim, K. Roszczewska, J. Tapia, F. Stockhardt, J. Gonzalez-Soler, J.-Y. Lim, M. Pollok, F. Kreuzer, C. Wang, L. Li, F. Guo, J. Gu, D. Pal, <strong><u>P. Farmanifard</u></strong>, et al., <em>"Iris Liveness Detection Competition (LivDet-Iris) – The 2025 Edition"</em>, Proc. of IEEE International Joint Conference on Biometrics (IJCB), 2025. <a href="https://github.com/CVRL/livdet-iris-2025?tab=readme-ov-file" target="_blank" rel="noopener">[Competition]</a></p>
        </div>
        <div class="pub-item">
          <p>[2025] Redwan Sony, <strong><u>P. Farmanifard</u></strong>, Arun Ross, Anil K Jain, <a href="https://arxiv.org/abs/2507.03541" target="_blank" rel="noopener"><em>"Foundation versus Domain-specific Models: Performance Comparison, Fusion, and Explainability in Face Recognition"</em></a>, IEEE/CVF International Conference on Computer Vision Workshops (ICCVW), (Honolulu, Hawai'i), October 2025.</p>
        </div>
        <div class="pub-item">
          <p>[2025] Redwan Sony, <strong><u>P. Farmanifard</u></strong>, Hamzeh Alzwairy, Nitish Shukla, Arun Ross, <a href="https://arxiv.org/abs/2505.24214" target="_blank" rel="noopener"><em>"Benchmarking Foundation Models for Zero-Shot Biometric Tasks"</em></a>, [Under Review].</p>
        </div>
        <div class="pub-item">
          <p>[2025] Rasel Ahmed Bhuyian, <strong><u>P. Farmanifard</u></strong>, Renu Sharma, Andrey Kuehlkamp, Aidan Boyd, Patrick J Flynn, Kevin W Bowyer, Arun Ross, Dennis Chute, Adam Czajka, <a href="https://ieeexplore.ieee.org/abstract/document/11063436" target="_blank" rel="noopener"><em>"Beyond Mortality: Advancements in Post-Mortem Iris Recognition through Data Collection and Computer-Aided Forensic Examination"</em></a>, IEEE Transactions on Biometrics, Behavior, and Identity Science.</p>
        </div>
        <div class="pub-item">
          <p>[2024] <strong><u>P. Farmanifard</u></strong> and Arun Ross, <a href="https://arxiv.org/abs/2408.04868" target="_blank" rel="noopener"><em>"ChatGPT Meets Iris Biometrics"</em></a>, Proc. of International Joint Conference on Biometrics (IJCB), (Buffalo, USA), September 2024 [Oral].</p>
        </div>
        <div class="pub-item">
          <p>[2024] <strong><u>P. Farmanifard</u></strong> and Arun Ross, <a href="https://arxiv.org/abs/2402.06497" target="_blank" rel="noopener"><em>"Iris-SAM: Iris Segmentation Using a Foundation Model"</em></a>, Proc. of 4th International Conference on Pattern Recognition and Artificial Intelligence (ICPRAI), (Jeju Island, South Korea), July 2024 [Oral].</p>
        </div>
      </div>
    </section>

    <!-- Projects with framework images -->
    <section class="section" id="projects">
      <h3>Projects</h3>

      <div class="project">
        <h4>Contact Lens Detection</h4>
        <div class="proj-body">
          <figure class="frame" data-lightbox="images/pad_framework.png" aria-label="PAD framework image">
            <img src="images/pad_framework.png" alt="PAD framework diagram (clear vs patterned lens detection)" loading="lazy">
          </figure>
          <div>
            <p>Currently, working on improving the detection of contact lenses in iris images to prevent fraud in iris recognition systems. Specifically, I use deep learning models to distinguish between natural irises and those altered by clear or patterned contact lenses, aiming to boost the security of these biometric systems.</p>
          </div>
        </div>
      </div>

      <div class="project">
        <h4>ChatGPT Meets Iris Biometrics</h4>
        <div class="proj-body">
          <figure class="frame" data-lightbox="images/chatgpt_iris_framework.png" aria-label="ChatGPT-Iris framework image">
            <img src="images/chatgpt_iris_framework.png" alt="Framework diagram for ChatGPT-based iris verification" loading="lazy">
          </figure>
          <div>
            <p>This study utilizes the advanced capabilities of the GPT4 multimodal Large Language Model (LLM) to explore its potential in iris recognition — a field less common and more specialized than face recognition. By focusing on this niche yet crucial area, we investigate how well AI tools like ChatGPT can understand and analyze iris images. Through a
            series of meticulously designed experiments employing a zero-shot learning approach, the capabilities of ChatGPT-4 was assessed across various challenging conditions including diverse datasets, presentation attacks, occlusions such as glasses, and other real-world variations. The findings
            convey ChatGPT-4’s remarkable adaptability and precision, revealing its proficiency in identifying distinctive iris features, while also detecting subtle effects like makeup on iris recognition. A comparative analysis with Gemini Advanced – Google’s AI model – highlighted ChatGPT-4’s
            better performance and user experience in complex iris analysis tasks. This research not only validates the use of LLMs for specialized biometric applications but also emphasizes the importance of nuanced query framing and interaction design in extracting significant insights from biometric data.
            Our findings suggest a promising path for future research and the development of more adaptable, efficient, robust and interactive biometric security solutions.</p>
          </div>
        </div>
      </div>

      <div class="project">
        <h4>Iris-SAM: Iris Segmentation Using a Foundation Model</h4>
        <div class="proj-body">
          <figure class="frame" data-lightbox="images/iris_sam_framework.png" aria-label="Iris-SAM framework image">
            <img src="images/iris_sam_framework.png" alt="Iris-SAM segmentation pipeline diagram" loading="lazy">
          </figure>
          <div>
            <p>Iris segmentation is a critical component of an iris biometric system and it involves extracting the annular iris region from an ocular image. In this work, we develop a pixel-level iris segmentation model from a foundation model, viz., Segment Anything Model (SAM), that
            has been successfully used for segmenting arbitrary objects. The primary contribution of this work lies in the integration of different loss functions during the fine-tuning of SAM on ocular images. In particular, the importance of Focal Loss is borne out in the fine-tuning process since it
            strategically addresses the class imbalance problem (i.e., iris versus noniris pixels). Experiments on ND-IRIS-0405, CASIA-Iris-Interval-v3, and IIT-Delhi-Iris datasets convey the efficacy of the trained model for the task of iris segmentation. For instance, on the ND-IRIS-0405 dataset, an
            average segmentation accuracy of 99.58% was achieved, compared to the best baseline performance of 89.75%.</p>
            <p><a href="https://github.com/ParisaFarmanifard/Iris-SAM" target="_blank" rel="noopener">View on GitHub</a></p>
          </div>
        </div>
      </div>

      <div class="project">
        <h4>Cross-Spectral Face Recognition Using Generative Adversarial Networks (GANs)</h4>
        <div class="proj-body">
          <figure class="frame" data-lightbox="images/FR_framework.png" aria-label="Cross-spectral framework image">
            <img src="images/FR_framework.png" alt="Cross-spectral face recognition framework diagram" loading="lazy">
          </figure>
          <div>
            <p>Cross-spectral Face Recognition (CFR) refers to the task of identifying matching features between two images of the same person captured in two different spectral bands. This involves recognizing faces across different imaging modalities such as visible, infrared, or depth maps. In this study, 
            we used ArcFace to calculate match scores between images taken from different spectral bands. We first computed similarity scores between original images of the same identity captured in SWIR and VIS, and then compared them with similarities between synthesized VIS images and the 
            original VIS/SWIR images using SG-GAN. Our results showed that using ArcFace led to the best performance for both synthesized and original images. This was because the similarity scores were well-separated in the histogram, indicating high discrimination between different identities. However, the performance of outdoor image analysis can be improved by applying image enhancement methods that address issues such as low contrast, noise, or illumination variations.</p>
          </div>
        </div>
      </div>

    </section>

    <!-- Posters -->
    <section class="section">
      <h3>Poster Presentations</h3>
      <ul>
        <li>"ChatGPT Meets Iris Biometrics", IJCB 2024 (Buffalo, NY).</li>
        <li>"Iris-SAM: Iris Segmentation Using a Foundation Model", Engineering Graduate Research Symposium 2024 (MSU).</li>
        <li>"Cross-Spectral Face Recognition Using a Semantic-Guided GAN", Engineering Graduate Research Symposium 2023 (MSU).</li>
      </ul>
    </section>

    <!-- Education -->
    <section class="section">
      <h3>Education</h3>
      <ul>
        <li>[2021 - Present] Doctor of Philosophy (Ph.D.), Computer Science, Michigan State University, USA.</li>
        <li>[2021 - 2023] Master of Science (MS), Computer Science, Michigan State University, USA.</li>
        <li>[2013 - 2017] Bachelor of Engineering (B.E.), IT Engineering, MU.</li>
      </ul>
    </section>

    <!-- Internship -->
    <section class="section">
      <h3>Internship Experience</h3>
      <ul>
        <li>In the summer of 2023, I had an internship experience at <a href="https://www.aep.com/">American Electric Power (AEP)</a>, working with their emerging technology group. My role involved a project on <em>"drone-based pole detection"</em>, collaborating closely with a computer vision and machine learning group. During my internship, I have worked with various detection methods, including <strong>Detectron2, Yolov4, Yolov8</strong>, and the <strong>Segment Anything Model (SAM)</strong>, primarily focusing on drone images.</li>
      </ul>
    </section>

    <footer>
      <p>Contact: <a href="mailto:parisa.farmanifard@gmail.com"><em>parisa.farmanifard@gmail.com</em></a></p>
    </footer>
  </div>

  <!-- Minimal lightbox logic -->
  <dialog class="lightbox" id="lightbox">
    <img class="lightbox-img" id="lightboxImg" alt="Framework preview">
  </dialog>
  <script>
    // Click any .frame to open its image in a native <dialog> lightbox
    const dialog = document.getElementById('lightbox');
    const dialogImg = document.getElementById('lightboxImg');
    document.querySelectorAll('.frame').forEach(frame => {
      const src = frame.getAttribute('data-lightbox');
      frame.addEventListener('click', () => {
        dialogImg.src = src;
        dialog.showModal();
      });
    });
    dialog.addEventListener('click', e => {
      const rect = dialogImg.getBoundingClientRect();
      const outside = e.clientX < rect.left || e.clientX > rect.right || e.clientY < rect.top || e.clientY > rect.bottom;
      if(outside){ dialog.close(); dialogImg.src = ""; }
    });
    document.addEventListener('keydown', e => { if(e.key === 'Escape' && dialog.open){ dialog.close(); dialogImg.src=""; } });
  </script>
</body>
</html>
